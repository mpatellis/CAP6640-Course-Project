{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "postal-voice",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/opt/spark')\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env')\n",
    "access = os.environ.get('AWS_ACCESS')\n",
    "secret = os.environ.get('AWS_SECRET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "latter-tackle",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.sql.functions import col, size, udf\n",
    "\n",
    "conf = SparkConf() \\\n",
    "    .set(\"fs.s3a.awsAccessKeyId\", access) \\\n",
    "    .set(\"fs.s3a.awsSecretAccessKey\", secret) \\\n",
    "    .set(\"fs.s3a.endpoint\", \"s3.us-east-1.amazonaws.com\") \\\n",
    "    .set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .set(\"fs.s3a.impl\",\"org.apache.hadoop.fs.s3native.NativeS3FileSystem\") \\\n",
    "    .set(\"com.amazonaws.services.s3.enableV4\", \"true\") \\\n",
    "    .set(\"spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version\", \"2\") \\\n",
    "    .set(\"spark.hadoop.fs.s3a.fast.upload\",\"true\") \\\n",
    "    .set(\"spark.sql.parquet.filterPushdown\", \"true\")\n",
    "\n",
    "spark = SparkSession.builder.appName('data-cleaning').config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "cleared-alert",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "from sparknlp.base import Finisher, DocumentAssembler\n",
    "from sparknlp.annotator import (Tokenizer, Normalizer,\n",
    "                                LemmatizerModel, StopWordsCleaner)\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "regional-sleeping",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = ''\n",
    "df = spark.read.parquet(filename).drop('geo', 'coordinates', 'place', 'Unnamed: 0', 'Unnamed: 0.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "filled-gather",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n |-- id: long (nullable = true)\n |-- full_text: string (nullable = true)\n |-- retweet_count: double (nullable = true)\n |-- favorite_count: double (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "central-mineral",
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessor as p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "defensive-worry",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanTweet(text):\n",
    "    p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.SMILEY, p.OPT.NUMBER, p.OPT.MENTION)\n",
    "    text = p.clean(text)\n",
    "    return text\n",
    "tweetPreprocessor_udf = udf(cleanTweet, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractHashtag(text):\n",
    "    result = []\n",
    "    parse = p.parse(text)\n",
    "\n",
    "    if parse.hashtags == None:\n",
    "        return result\n",
    "    \n",
    "    for i in range(len(parse.hashtags)):\n",
    "        result.append(parse.hashtags[i].match[1:])\n",
    "    return result\n",
    "\n",
    "extractHashtag_udf = udf(extractHashtag, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('hashtags', extractHashtag_udf('full_text'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "dedicated-improvement",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------+\n|hashtags|\n+--------+\n|      []|\n|      []|\n|      []|\n|      []|\n|      []|\n|      []|\n|      []|\n|      []|\n|      []|\n|      []|\n|      []|\n|      []|\n|      []|\n|      []|\n|      []|\n|      []|\n|      []|\n|      []|\n|      []|\n|      []|\n+--------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('clean', tweetPreprocessor_udf('full_text'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "passive-complex",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The nltk stopword won't download onto emr without some setup so it's just easier to copy and paste\n",
    "stopwords_list = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'youre', 'youve', 'youll', 'youd', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'shes', 'her', 'hers', 'herself', 'it', 'its', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'thatll', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'dont', 'should', 'shouldve', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'arent', 'couldn', 'couldnt', 'didn', 'didnt', 'doesn', 'doesnt', 'hadn', 'hadnt', 'hasn', 'hasnt', 'haven', 'havent', 'isn', 'isnt', 'ma', 'mightn', 'mightnt', 'mustn', 'mustnt', 'needn', 'neednt', 'shan', 'shant', 'shouldn', 'shouldnt', 'wasn', 'wasnt', 'weren', 'werent', 'won', 'wont', 'wouldn', 'wouldnt', 'like', 'hes', 'let', 'lot', 'ok', 'yes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "flush-premises",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "documentAssembler = DocumentAssembler() \\\n",
    "    .setInputCol('clean') \\\n",
    "    .setOutputCol('document') \\\n",
    "    .setCleanupMode('shrink_full')\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols(['document']) \\\n",
    "    .setOutputCol('token')\n",
    "\n",
    "normalizer = Normalizer() \\\n",
    "    .setInputCols(['token']) \\\n",
    "    .setOutputCol('normalized') \\\n",
    "    .setLowercase(True)\n",
    "\n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "    .setInputCols(['normalized']) \\\n",
    "    .setOutputCol('normalized_no_stop') \\\n",
    "    .setCaseSensitive(False) \\\n",
    "    .setStopWords(stopwords_list)\n",
    "\n",
    "lemmatizer = LemmatizerModel.pretrained() \\\n",
    "    .setInputCols(['normalized_no_stop']) \\\n",
    "    .setOutputCol('lemma')\n",
    "\n",
    "finisher = Finisher() \\\n",
    "     .setInputCols(['lemma']) \\\n",
    "     .setCleanAnnotations(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "alike-effects",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline() \\\n",
    "     .setStages([\n",
    "           documentAssembler,\n",
    "           tokenizer,\n",
    "           normalizer,\n",
    "           stopwords_cleaner,\n",
    "           lemmatizer,\n",
    "           finisher\n",
    "     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "sophisticated-string",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pipeline.fit(df).transform(df)\n",
    "df = df.drop('clean').withColumnRenamed('finished_lemma', 'clean_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "honey-brighton",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.where(size(col('clean_text')) > 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|clean_text                                                                                                                                                        |\n+------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|[impeach, mikepence, murder, +, american, many, supporter]                                                                                                        |\n|[kind, gloss, domestic, terrorism, blue, livestrump, supporter, trumpsterrorist, revoke, trumpfailure]                                                            |\n|[let, start, prosecute, white, house, violation, hatch, act]                                                                                                      |\n|[let, talk, troop, ready, go, mayor, govenor, ask, refuse, cut, back, police, that, problem]                                                                      |\n|[fallacy, violation, pauls, teach, regard, brother, interact, problem, will, knowledge, personally, see, fruit, pass, judgement, study, bible, daily, fruit, know]|\n|[nobody, give, shit, ratings, orange, con, man]                                                                                                                   |\n|[people, even, live, btw, come, look, fight, theyre, racist, piece, shit, embolden]                                                                               |\n|[show, tweet, agree, trump, tweet, unless, click, see, response, another, threadcrazy]                                                                            |\n|[quite, artist, victor, take, art, class]                                                                                                                         |\n|[fine, probably, constipate, unbearable, cant, poop]                                                                                                              |\n|[need, order, amp, law, president, please, invoke, insurrection, act, support, insurrectionactnow]                                                                |\n|[mt, america, control, much, bad, obama, video, counterprotestor, gun, blm, protest, today, florida, outside, capitol]                                            |\n|[comply, police, try, everything, else, suspect, keep, go, become, imminent, threat, need, stop, number, bullet, nothing]                                         |\n|[one, dangerous, caravan, warn, we, people, come, mile, away, start, trouble]                                                                                     |\n|[hey, eric, family, vote, person, guess, vote, mail, republican, anyone, rig, election, traitor, go, hide, bunker, together]                                      |\n|[pack, lie, go, smoke, much, bull, line, attack, joe, liarandcretin, income]                                                                                      |\n|[gonna, lot, liberal, tear, november]                                                                                                                             |\n|[mean, lil, chip, run, person, idaho, crappy, taco, place]                                                                                                        |\n|[senator, declare, publicly, kid, contract, covid, appease]                                                                                                       |\n|[cause, blood, hand, along, death, american, due, covid, murderer]                                                                                                |\n+------------------------------------------------------------------------------------------------------------------------------------------------------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.select('clean_text').show(truncate=False)"
   ]
  },
  {
   "source": [
    "Writing will take forever"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "awful-dressing",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = ''\n",
    "df.write.parquet(filename, mode='overwrite')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python385jvsc74a57bd04ef8975e22676df32c930aae12a41200d93f0ab6e5e4fbe73ecc9ad618aa940e",
   "display_name": "Python 3.8.5 64-bit ('venv')"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3",
   "version": "3.8.5-final"
  },
  "metadata": {
   "interpreter": {
    "hash": "4ef8975e22676df32c930aae12a41200d93f0ab6e5e4fbe73ecc9ad618aa940e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}