{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd04ef8975e22676df32c930aae12a41200d93f0ab6e5e4fbe73ecc9ad618aa940e",
   "display_name": "Python 3.8.5 64-bit ('venv')"
  },
  "metadata": {
   "interpreter": {
    "hash": "4ef8975e22676df32c930aae12a41200d93f0ab6e5e4fbe73ecc9ad618aa940e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/opt/spark')\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env')\n",
    "access = os.environ.get('AWS_ACCESS')\n",
    "secret = os.environ.get('AWS_SECRET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "from pyspark.ml.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vector, Vectors\n",
    "from pyspark.ml.feature import CountVectorizer , IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf() \\\n",
    "    .set(\"fs.s3a.awsAccessKeyId\", access) \\\n",
    "    .set(\"fs.s3a.awsSecretAccessKey\", secret) \\\n",
    "    .set(\"fs.s3a.endpoint\", \"s3.us-east-1.amazonaws.com\") \\\n",
    "    .set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .set(\"fs.s3a.impl\",\"org.apache.hadoop.fs.s3native.NativeS3FileSystem\") \\\n",
    "    .set(\"com.amazonaws.services.s3.enableV4\", \"true\")\n",
    "\n",
    "spark = SparkSession.builder.master('LDA').appName('cool').config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------------+--------------------+-------------+--------------+--------------------+\n|                 id|           full_text|retweet_count|favorite_count|          clean_text|\n+-------------------+--------------------+-------------+--------------+--------------------+\n|1300070747059167233|@kmiranda1973 @Mi...|          0.0|           2.0|[kmiranda, militi...|\n|1300070747453427713|@realDonaldTrump ...|          0.0|           0.0|[realdonaldtrump,...|\n|1300070747566755845|@realDonaldTrump ...|          0.0|           1.0|[realdonaldtrump,...|\n|1300070748116131840|@EricTrump @realD...|          0.0|           1.0|[erictrump, reald...|\n|1300070748359454721|Impeached @realdo...|          0.0|           1.0|[impeach, realdon...|\n+-------------------+--------------------+-------------+--------------+--------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# filename = 's3a://patellism/processed_data/2020-08-cleaned.parquet.snappy/part-00004-d2d9c5cf-46de-47d9-86df-28fefd1709e5-c000.snappy.parquet'\n",
    "filename = '2020-08-cleaned-small.snappy.parquet'\n",
    "df = spark.read.parquet(filename).drop('geo', 'coordinates', 'place')\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_idf = CountVectorizer(inputCol='clean_text', outputCol='raw_features', vocabSize=10000, minDF=35)\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "lda_tfidf = LDA(k=10, maxIter=2)\n",
    "\n",
    "tfidf_pipeline = Pipeline(stages=[cv_idf, idf, lda_tfidf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_tf = CountVectorizer(inputCol='clean_text', outputCol='features', vocabSize=10000, minDF=35)\n",
    "lda_tf = LDA(k=10, maxIter=2)\n",
    "\n",
    "tf_pipeline = Pipeline(stages=[cv, lda])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three groupings\n",
    "#   Ungrouped\n",
    "#   Time\n",
    "#   Hashtags\n",
    "#       Need to determine similarity metrics and method for tweets with no hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tfidf_pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = model.stages[-1].describeTopics(maxTermsPerTopic=5)\n",
    "vocabArray = model.stages[0].vocabulary\n",
    "\n",
    "def covertToWord(indices):\n",
    "    result = []\n",
    "    for i in indices:\n",
    "        result.append(vocabArray[i])\n",
    "    return result\n",
    "\n",
    "udf_convertToWord = udf(covertToWord, ArrayType(StringType()))\n",
    "topics = topics.withColumn('word', udf_convertToWord('termIndices'))\n",
    "topics.select('word').show(truncate=False)"
   ]
  }
 ]
}