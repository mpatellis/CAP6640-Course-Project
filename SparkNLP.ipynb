{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SparkNLP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGGVrGR-VtZ9",
        "outputId": "92cdd8a7-03bd-470f-c8fc-a2a14a3264ba"
      },
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/b0/9d6860891ab14a39d4bddf80ba26ce51c2f9dc4805e5c6978ac0472c120a/pyspark-3.1.1.tar.gz (212.3MB)\n",
            "\u001b[K     |████████████████████████████████| 212.3MB 70kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 44.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.1-py2.py3-none-any.whl size=212767604 sha256=b2efe330e1e5010fc9d489c474c6bdebc4d49890f0fac3c4aec0b8a06fca51b8\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/90/c0/01de724414ef122bd05f056541fb6a0ecf47c7ca655f8b3c0f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.1\n",
            "The following additional packages will be installed:\n",
            "  openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra\n",
            "  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
            "  fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 2 newly installed, 0 to remove and 31 not upgraded.\n",
            "Need to get 36.5 MB of archives.\n",
            "After this operation, 143 MB of additional disk space will be used.\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "(Reading database ... 160983 files and directories currently installed.)\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u282-b08-0ubuntu1~18.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u282-b08-0ubuntu1~18.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u282-b08-0ubuntu1~18.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u282-b08-0ubuntu1~18.04) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u282-b08-0ubuntu1~18.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u282-b08-0ubuntu1~18.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "mZvq1_91YpE7",
        "outputId": "83b2da58-0e89-49ff-e000-9fbbf63afe41"
      },
      "source": [
        "''' \n",
        "\n",
        "function ConnectButton(){\n",
        "    console.log(\"Connect pushed\"); \n",
        "    document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click() \n",
        "}\n",
        "setInterval(ConnectButton,60000);\n",
        "\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' \\n\\nfunction ConnectButton(){\\n    console.log(\"Connect pushed\"); \\n    document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click() \\n}\\nsetInterval(ConnectButton,60000);\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvP_CJ-JJGUp",
        "outputId": "35e2fed7-fc87-41ef-da48-831c244b9672"
      },
      "source": [
        "# Spark NLP download\n",
        "!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-17 22:01:12--  http://setup.johnsnowlabs.com/colab.sh\n",
            "Resolving setup.johnsnowlabs.com (setup.johnsnowlabs.com)... 51.158.130.26\n",
            "Connecting to setup.johnsnowlabs.com (setup.johnsnowlabs.com)|51.158.130.26|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh [following]\n",
            "--2021-04-17 22:01:13--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1594 (1.6K) [text/plain]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "-                     0%[                    ]       0  --.-KB/s               setup Colab for PySpark 3.0.2 and Spark NLP 3.0.1\n",
            "-                   100%[===================>]   1.56K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2021-04-17 22:01:13 (1.13 MB/s) - written to stdout [1594/1594]\n",
            "\n",
            "\u001b[K     |████████████████████████████████| 204.8MB 62kB/s \n",
            "\u001b[K     |████████████████████████████████| 153kB 34.1MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRRqLgHrJRBP",
        "outputId": "8dede520-7b38-4771-8318-1909d56ca70d"
      },
      "source": [
        "# Find Spark nlp to confirm successful download\n",
        "import sparknlp\n",
        "spark = sparknlp.start()\n",
        "\n",
        "print(\"Spark NLP version: {}\".format(sparknlp.version()))\n",
        "print(\"Apache Spark version: {}\".format(spark.version))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Spark NLP version: 3.0.1\n",
            "Apache Spark version: 3.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZf-bFqNKzxq"
      },
      "source": [
        "# Download jdk\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXFW6x_6Koz9"
      },
      "source": [
        "# Download spark and hadoop\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcJKQu-hK20K"
      },
      "source": [
        "# Extract spark \n",
        "!tar xf spark-3.1.1-bin-hadoop2.7.tgz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0BPCJnDK5OI"
      },
      "source": [
        "# Library for finding spark download\n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0x4OfK8K_lD"
      },
      "source": [
        "# Change env variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop2.7\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "HW6dqrfELJzq",
        "outputId": "c48d9093-b2c4-4cbe-e594-d251bd08badb"
      },
      "source": [
        "# Find spark location\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/spark-3.1.1-bin-hadoop2.7'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGWrMGipM23k",
        "outputId": "2110bdc9-f0c1-4054-c360-e2dabc7a6caf"
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "get_ipython().system_raw('./ngrok http 4050 &')\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-17 22:04:25--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 54.235.108.207, 34.194.108.77, 52.204.190.140, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|54.235.108.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14746350 (14M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  14.06M  7.01MB/s    in 2.0s    \n",
            "\n",
            "2021-04-17 22:04:28 (7.01 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [14746350/14746350]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "  File \"/usr/lib/python3.7/json/__init__.py\", line 296, in load\n",
            "    parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n",
            "  File \"/usr/lib/python3.7/json/__init__.py\", line 348, in loads\n",
            "    return _default_decoder.decode(s)\n",
            "  File \"/usr/lib/python3.7/json/decoder.py\", line 337, in decode\n",
            "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
            "  File \"/usr/lib/python3.7/json/decoder.py\", line 355, in raw_decode\n",
            "    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n",
            "json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkbaxbYWUpTI"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkConf\n",
        "from pyspark.sql.types import StringType\n",
        "from pyspark.sql.functions import col, size, udf\n",
        "\n",
        "from pyspark.ml.clustering import LDA, LDAModel\n",
        "from pyspark.mllib.linalg import Vector, Vectors\n",
        "from pyspark.ml.feature import CountVectorizer , IDF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNLiv1CZWOuk"
      },
      "source": [
        "access = \"AKIAW2VJQN34WBUWDLIH\"\n",
        "secret = \"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\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNs0oWRxWGMT"
      },
      "source": [
        "\n",
        "conf = SparkConf() \\\n",
        "    .set(\"spark.ui.port\", \"4050\") \\\n",
        "    .set(\"fs.s3a.awsAccessKeyId\", access) \\\n",
        "    .set(\"fs.s3a.awsSecretAccessKey\", secret) \\\n",
        "    .set(\"fs.s3a.endpoint\", \"s3.us-east-1.amazonaws.com\") \\\n",
        "    .set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
        "    .set(\"fs.s3a.impl\",\"org.apache.hadoop.fs.s3native.NativeS3FileSystem\") \\\n",
        "    .set(\"com.amazonaws.services.s3.enableV4\", \"true\") \\\n",
        "    .set(\"spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version\", \"2\") \\\n",
        "    .set(\"spark.hadoop.fs.s3a.fast.upload\",\"true\") \\\n",
        "    .set(\"spark.sql.parquet.filterPushdown\", \"true\")\n",
        "\n",
        "spark = SparkSession.builder.appName('data-cleaning').config(conf=conf).getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "ezsgoU04XeJq",
        "outputId": "4f7055f7-4129-4828-bf39-df3384608cbd"
      },
      "source": [
        "spark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://acc85cb307ce:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.0.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Spark NLP</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f8fca1e5d10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-HJ_ITvjZ1-",
        "outputId": "90e21e0b-738d-4988-d6da-7c1ba223a6ad"
      },
      "source": [
        "# S3 Client\n",
        "!pip install boto3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/b3/8c889dd3d5ae47a9c4468cc20ef980adc4a16f06f0937ab33f78b58b5eda/boto3-1.17.53-py2.py3-none-any.whl (131kB)\n",
            "\r\u001b[K     |██▌                             | 10kB 15.3MB/s eta 0:00:01\r\u001b[K     |█████                           | 20kB 10.2MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 30kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████                      | 40kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 51kB 4.2MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 61kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 71kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 81kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 92kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 102kB 4.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 112kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 122kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 4.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from boto3) (0.3.7)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.21.0,>=1.20.53 in /usr/local/lib/python3.7/dist-packages (from boto3) (1.20.53)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.53->boto3) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.53->boto3) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.53->boto3) (1.15.0)\n",
            "Installing collected packages: boto3\n",
            "Successfully installed boto3-1.17.53\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpiLq0I-RvxS"
      },
      "source": [
        "AWS_ACCESS='AKIAW2VJQN34WBUWDLIH'\n",
        "AWS_SECRET='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'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yedPi1j6jk_q"
      },
      "source": [
        "import boto3\n",
        "\n",
        "s3client = boto3.client(\n",
        "    service_name='s3',\n",
        "    region_name='us-east-1',\n",
        "    aws_access_key_id=AWS_ACCESS,\n",
        "    aws_secret_access_key=AWS_SECRET\n",
        ")\n",
        "\n",
        "s3 = boto3.resource(\n",
        "    service_name='s3',\n",
        "    region_name='us-east-1',\n",
        "    aws_access_key_id=AWS_ACCESS,\n",
        "    aws_secret_access_key=AWS_SECRET\n",
        ")\n",
        "buck = s3.Bucket('patellism')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhlPKuRnBZBI",
        "outputId": "c3bf5c66-691b-428c-ff85-43eb77a1cdcb"
      },
      "source": [
        "buck"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "s3.Bucket(name='patellism')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4svd009azMv"
      },
      "source": [
        "buck.download_file('full_test_set.jsonl', 'full_test_set.jsonl')\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p38mZE0lQaoD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b59e2d4c-0f51-47e8-a3a3-6483cacc14e8"
      },
      "source": [
        "df = spark.read.json('full_test_set.jsonl').drop('geo', 'coordinates', 'place')\n",
        "df.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------+--------------------+------------------+--------------------+-----------------+--------------+---------+--------------------+-------------------+-------------------+-----------------------+---------------------+-------------------------+-------------------+-----------------------+---------------+----+------------------+--------------------+-------------------+--------------------+-----------------------+-------------+---------+--------------------+--------------------+---------+--------------------+------------------+---------------------+--------------+\n",
            "|contributors|          created_at|display_text_range|            entities|extended_entities|favorite_count|favorited|           full_text|                 id|             id_str|in_reply_to_screen_name|in_reply_to_status_id|in_reply_to_status_id_str|in_reply_to_user_id|in_reply_to_user_id_str|is_quote_status|lang|possibly_sensitive|       quoted_status|   quoted_status_id|quoted_status_id_str|quoted_status_permalink|retweet_count|retweeted|    retweeted_status|              source|truncated|                user|withheld_copyright|withheld_in_countries|withheld_scope|\n",
            "+------------+--------------------+------------------+--------------------+-----------------+--------------+---------+--------------------+-------------------+-------------------+-----------------------+---------------------+-------------------------+-------------------+-----------------------+---------------+----+------------------+--------------------+-------------------+--------------------+-----------------------+-------------+---------+--------------------+--------------------+---------+--------------------+------------------+---------------------+--------------+\n",
            "|        null|Wed Jan 01 00:25:...|          [0, 140]|[[],, [], [], [[2...|             null|             0|    false|RT @GOPChairwoman...|1212167934652637185|1212167934652637185|                   null|                 null|                     null|               null|                   null|          false|  en|              null|                null|               null|                null|                   null|         7322|    false|[,, Tue Dec 31 20...|<a href=\"http://t...|    false|[false, Mon Jun 2...|              null|                 null|          null|\n",
            "|        null|Wed Jan 01 00:45:...|          [0, 140]|[[],, [], [], [[1...|             null|             0|    false|RT @parscale: “We...|1212173031080177664|1212173031080177664|                   null|                 null|                     null|               null|                   null|          false|  en|              null|                null|               null|                null|                   null|         3654|    false|[,, Tue Dec 31 23...|<a href=\"http://t...|    false|[false, Wed May 2...|              null|                 null|          null|\n",
            "|        null|Wed Jan 01 01:47:...|          [17, 97]|[[],, [], [[twitt...|             null|             0|    false|@realDonaldTrump ...|1212188473777676290|1212188473777676290|        realDonaldTrump|  1212184310389850119|      1212184310389850119|           25073877|               25073877|           true| und|             false|[,, Wed Jan 01 01...|1212188061007798273| 1212188061007798273|   [twitter.com/JW40...|            0|    false|                null|<a href=\"http://t...|    false|[false, Tue Aug 0...|              null|                 null|          null|\n",
            "|        null|Wed Jan 01 02:32:...|          [43, 71]|[[],, [], [], [[7...|             null|             0|    false|@KeneeBe @TimRuns...|1212199831835303936|1212199831835303936|                KeneeBe|  1212188784001003520|      1212188784001003520|           73364089|               73364089|          false|  en|              null|                null|               null|                null|                   null|            0|    false|                null|<a href=\"https://...|    false|[false, Wed Jan 0...|              null|                 null|          null|\n",
            "|        null|Wed Jan 01 03:00:...|         [55, 144]|[[],, [], [], [[3...|             null|             0|    false|@DanDee1948 @sunr...|1212207061691437056|1212207061691437056|             DanDee1948|  1212205308216401920|      1212205308216401920|         3289332115|             3289332115|          false|  en|              null|                null|               null|                null|                   null|            0|    false|                null|<a href=\"http://t...|    false|[false, Sat Sep 2...|              null|                 null|          null|\n",
            "+------------+--------------------+------------------+--------------------+-----------------+--------------+---------+--------------------+-------------------+-------------------+-----------------------+---------------------+-------------------------+-------------------+-----------------------+---------------+----+------------------+--------------------+-------------------+--------------------+-----------------------+-------------+---------+--------------------+--------------------+---------+--------------------+------------------+---------------------+--------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWSpOsajCNBe"
      },
      "source": [
        "Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26Do66kCCTGl",
        "outputId": "b9ea6eb3-b867-494b-8c59-e74c883e5f7a"
      },
      "source": [
        "!pip install tweet-preprocessor"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tweet-preprocessor\n",
            "  Downloading https://files.pythonhosted.org/packages/17/9d/71bd016a9edcef8860c607e531f30bd09b13103c7951ae73dd2bf174163c/tweet_preprocessor-0.6.0-py3-none-any.whl\n",
            "Installing collected packages: tweet-preprocessor\n",
            "Successfully installed tweet-preprocessor-0.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8i_qwg8CW5G"
      },
      "source": [
        "def cleanTweet(text):\n",
        "    p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.SMILEY, p.OPT.NUMBER, p.OPT.MENTION)\n",
        "    text = p.clean(text)\n",
        "    return text\n",
        "tweetPreprocessor_udf = udf(cleanTweet, StringType())\n",
        "\n",
        "def extractHashtag(text):\n",
        "    result = []\n",
        "    parse = p.parse(text)\n",
        "\n",
        "    if parse.hashtags == None:\n",
        "        return result\n",
        "    \n",
        "    for i in range(len(parse.hashtags)):\n",
        "        result.append(parse.hashtags[i].match[1:])\n",
        "    return result\n",
        "\n",
        "extractHashtag_udf = udf(extractHashtag, ArrayType(StringType()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGmGZ0F_CPm3"
      },
      "source": [
        "import sparknlp\n",
        "from sparknlp.base import Finisher, DocumentAssembler\n",
        "from sparknlp.annotator import (Tokenizer, Normalizer,\n",
        "                                LemmatizerModel, StopWordsCleaner)\n",
        "from sparknlp.pretrained import PretrainedPipeline\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "import preprocessor as p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af3um1kuCaTE"
      },
      "source": [
        "df = df.withColumn('hashtags', extractHashtag_udf('full_text'))\n",
        "df = df.withColumn('clean', tweetPreprocessor_udf('full_text'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KwRIAfTCc7q"
      },
      "source": [
        "# The nltk stopword won't download onto emr without some setup so it's just easier to copy and paste\n",
        "stopwords_list = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'youre', 'youve', 'youll', 'youd', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'shes', 'her', 'hers', 'herself', 'it', 'its', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'thatll', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'dont', 'should', 'shouldve', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'arent', 'couldn', 'couldnt', 'didn', 'didnt', 'doesn', 'doesnt', 'hadn', 'hadnt', 'hasn', 'hasnt', 'haven', 'havent', 'isn', 'isnt', 'ma', 'mightn', 'mightnt', 'mustn', 'mustnt', 'needn', 'neednt', 'shan', 'shant', 'shouldn', 'shouldnt', 'wasn', 'wasnt', 'weren', 'werent', 'won', 'wont', 'wouldn', 'wouldnt', 'like', 'hes', 'let', 'lot', 'ok', 'yes']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tql1xiAUCfb5",
        "outputId": "dd43c59a-0e03-410c-a87f-395b9042fda7"
      },
      "source": [
        "documentAssembler = DocumentAssembler() \\\n",
        "    .setInputCol('clean') \\\n",
        "    .setOutputCol('document') \\\n",
        "    .setCleanupMode('shrink_full')\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols(['document']) \\\n",
        "    .setOutputCol('token')\n",
        "\n",
        "normalizer = Normalizer() \\\n",
        "    .setInputCols(['token']) \\\n",
        "    .setOutputCol('normalized') \\\n",
        "    .setLowercase(True)\n",
        "\n",
        "stopwords_cleaner = StopWordsCleaner() \\\n",
        "    .setInputCols(['normalized']) \\\n",
        "    .setOutputCol('normalized_no_stop') \\\n",
        "    .setCaseSensitive(False) \\\n",
        "    .setStopWords(stopwords_list)\n",
        "\n",
        "lemmatizer = LemmatizerModel.pretrained() \\\n",
        "    .setInputCols(['normalized_no_stop']) \\\n",
        "    .setOutputCol('lemma')\n",
        "\n",
        "finisher = Finisher() \\\n",
        "     .setInputCols(['lemma']) \\\n",
        "     .setCleanAnnotations(True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lemma_antbnc download started this may take some time.\n",
            "Approximate size to download 907.6 KB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88rjqM1MCjrf"
      },
      "source": [
        "pipeline = Pipeline() \\\n",
        "     .setStages([\n",
        "           documentAssembler,\n",
        "           tokenizer,\n",
        "           normalizer,\n",
        "           stopwords_cleaner,\n",
        "           lemmatizer,\n",
        "           finisher\n",
        "     ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jk6p7JDaClkW"
      },
      "source": [
        "df = pipeline.fit(df).transform(df)\n",
        "df = df.drop('clean').withColumnRenamed('finished_lemma', 'clean_text')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDAgtxFKCnQb"
      },
      "source": [
        "df = df.where(size(col('clean_text')) > 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2xPCb3-Cpik",
        "outputId": "70753b1c-fe18-45f0-a97e-b8b5be97d2e4"
      },
      "source": [
        "df.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------+--------------------+------------------+--------------------+--------------------+--------------+---------+--------------------+-------------------+-------------------+-----------------------+---------------------+-------------------------+-------------------+-----------------------+---------------+----+------------------+-------------+----------------+--------------------+-----------------------+-------------+---------+--------------------+--------------------+---------+--------------------+------------------+---------------------+--------------+--------+--------------------+\n",
            "|contributors|          created_at|display_text_range|            entities|   extended_entities|favorite_count|favorited|           full_text|                 id|             id_str|in_reply_to_screen_name|in_reply_to_status_id|in_reply_to_status_id_str|in_reply_to_user_id|in_reply_to_user_id_str|is_quote_status|lang|possibly_sensitive|quoted_status|quoted_status_id|quoted_status_id_str|quoted_status_permalink|retweet_count|retweeted|    retweeted_status|              source|truncated|                user|withheld_copyright|withheld_in_countries|withheld_scope|hashtags|          clean_text|\n",
            "+------------+--------------------+------------------+--------------------+--------------------+--------------+---------+--------------------+-------------------+-------------------+-----------------------+---------------------+-------------------------+-------------------+-----------------------+---------------+----+------------------+-------------+----------------+--------------------+-----------------------+-------------+---------+--------------------+--------------------+---------+--------------------+------------------+---------------------+--------------+--------+--------------------+\n",
            "|        null|Wed Jan 01 00:25:...|          [0, 140]|[[],, [], [], [[2...|                null|             0|    false|RT @GOPChairwoman...|1212167934652637185|1212167934652637185|                   null|                 null|                     null|               null|                   null|          false|  en|              null|         null|            null|                null|                   null|         7322|    false|[,, Tue Dec 31 20...|<a href=\"http://t...|    false|[false, Mon Jun 2...|              null|                 null|          null|      []|[rt, democrat, re...|\n",
            "|        null|Wed Jan 01 00:45:...|          [0, 140]|[[],, [], [], [[1...|                null|             0|    false|RT @parscale: “We...|1212173031080177664|1212173031080177664|                   null|                 null|                     null|               null|                   null|          false|  en|              null|         null|            null|                null|                   null|         3654|    false|[,, Tue Dec 31 23...|<a href=\"http://t...|    false|[false, Wed May 2...|              null|                 null|          null|      []|[rt, deal, lunati...|\n",
            "|        null|Wed Jan 01 03:00:...|         [55, 144]|[[],, [], [], [[3...|                null|             0|    false|@DanDee1948 @sunr...|1212207061691437056|1212207061691437056|             DanDee1948|  1212205308216401920|      1212205308216401920|         3289332115|             3289332115|          false|  en|              null|         null|            null|                null|                   null|            0|    false|                null|<a href=\"http://t...|    false|[false, Sat Sep 2...|              null|                 null|          null|      []|[thank, reveal, g...|\n",
            "|        null|Wed Jan 01 03:01:...|         [27, 259]|[[[[27, 41], Fake...|                null|             0|    false|@parscale @realDo...|1212207136899575808|1212207136899575808|               parscale|  1212158907336085505|      1212158907336085505|           17685258|               17685258|          false|  en|              null|         null|            null|                null|                   null|            0|    false|                null|<a href=\"http://t...|    false|[false, Fri Aug 0...|              null|                 null|          null|      []|[fakenewsmedia, d...|\n",
            "|        null|Wed Jan 01 03:02:...|         [17, 100]|[[[[69, 77], IMPO...|[[[, pic.twitter....|             0|    false|@realDonaldTrump ...|1212207367108075521|1212207367108075521|        realDonaldTrump|  1212184310389850119|      1212184310389850119|           25073877|               25073877|          false|  en|             false|         null|            null|                null|                   null|            0|    false|                null|<a href=\"http://t...|    false|[false, Wed Jun 2...|              null|                 null|          null|      []|[year, trump, man...|\n",
            "+------------+--------------------+------------------+--------------------+--------------------+--------------+---------+--------------------+-------------------+-------------------+-----------------------+---------------------+-------------------------+-------------------+-----------------------+---------------+----+------------------+-------------+----------------+--------------------+-----------------------+-------------+---------+--------------------+--------------------+---------+--------------------+------------------+---------------------+--------------+--------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zevABCEXdtnj"
      },
      "source": [
        "from sparknlp.pretrained import PretrainedPipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybUt8w8kdulr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f1e28c6-7ef2-4e23-f760-19401a8526dd"
      },
      "source": [
        "pipeline = PretrainedPipeline(\"analyze_sentimentdl_use_twitter\", lang=\"en\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "analyze_sentimentdl_use_twitter download started this may take some time.\n",
            "Approx size to download 935.1 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_cyOC5Rtdib"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1D9_Go5WM09Q"
      },
      "source": [
        "df_result = \\\n",
        "pipeline.transform(df.withColumnRenamed(\"clean_text\", \"text\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xSzmXXFSKd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f179d85a-b5f9-4df2-fb7f-8a68d0b7fbfd"
      },
      "source": [
        "df_result.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-debf91d4471a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    438\u001b[0m         \"\"\"\n\u001b[1;32m    439\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o753.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 14.0 failed 1 times, most recent failure: Lost task 0.0 in stage 14.0 (TID 29, acc85cb307ce, executor driver): org.apache.spark.SparkException: Failed to execute user defined function(DocumentAssembler$$Lambda$3206/661227163: (array<string>) => array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.subExpr_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:346)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:462)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:465)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.ClassCastException: scala.collection.mutable.WrappedArray$ofRef cannot be cast to java.lang.String\n\t... 18 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2135)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2154)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3627)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2697)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2697)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2904)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:337)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function(DocumentAssembler$$Lambda$3206/661227163: (array<string>) => array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.subExpr_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:459)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:346)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:462)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:465)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.lang.ClassCastException: scala.collection.mutable.WrappedArray$ofRef cannot be cast to java.lang.String\n\t... 18 more\n"
          ]
        }
      ]
    }
  ]
}